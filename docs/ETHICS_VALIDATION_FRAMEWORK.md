# Nethical Ethics Validation Framework

This framework provides a reproducible, quantitative methodology for evaluating safety and ethics detection quality, monitoring performance drift, and continuously improving detection capabilities through systematic feedback integration.

---

## Table of Contents

1. [Purpose & Scope](#purpose--scope)
2. [Violation Taxonomy](#violation-taxonomy)
3. [Dataset Versioning & Governance](#dataset-versioning--governance)
4. [Metrics & Targets](#metrics--targets)
5. [Threshold Tuning Strategy](#threshold-tuning-strategy)
6. [Drift Monitoring](#drift-monitoring)
7. [Reviewer Feedback Integration](#reviewer-feedback-integration)
8. [Reporting & Artifacts](#reporting--artifacts)
9. [Recalibration Workflow](#recalibration-workflow)
10. [Risk of Regression & Mitigations](#risk-of-regression--mitigations)

---

## Purpose & Scope

### Objectives

1. **Quantify Detection Quality**: Measure precision, recall, F1 score, and false negative rate across all violation categories
2. **Monitor Performance Drift**: Detect degradation in detection quality over time due to data distribution changes
3. **Enable Continuous Improvement**: Integrate human reviewer feedback to iteratively improve detection models
4. **Ensure Reproducibility**: Maintain versioned datasets and standardized evaluation procedures
5. **Support Accountability**: Generate comprehensive ethics reports for transparency and compliance

### Key Principles

- **Data-Driven**: All decisions backed by quantitative metrics
- **Transparency**: Methodology and results publicly documented
- **Continuous**: Ongoing evaluation, not one-time validation
- **Human-in-the-Loop**: Expert review complements automated detection
- **Fairness**: Bias analysis across demographic groups

---

## Violation Taxonomy

### Category Hierarchy

```
Ethics Violations
â”œâ”€â”€ Harmful Content
â”‚   â”œâ”€â”€ Violence & Physical Harm
â”‚   â”œâ”€â”€ Self-Harm & Suicide
â”‚   â”œâ”€â”€ Sexual Content (Non-Consensual)
â”‚   â”œâ”€â”€ Hate Speech
â”‚   â””â”€â”€ Dangerous Activities
â”‚
â”œâ”€â”€ Deception & Misinformation
â”‚   â”œâ”€â”€ Impersonation
â”‚   â”œâ”€â”€ Fraud & Scams
â”‚   â”œâ”€â”€ False Claims (Medical, Financial)
â”‚   â”œâ”€â”€ Deepfakes & Synthetic Media
â”‚   â””â”€â”€ Astroturfing & Coordinated Inauthentic Behavior
â”‚
â”œâ”€â”€ Privacy Violations
â”‚   â”œâ”€â”€ PII Disclosure (Unauthorized)
â”‚   â”œâ”€â”€ Doxing
â”‚   â”œâ”€â”€ Surveillance & Tracking
â”‚   â”œâ”€â”€ Data Breach & Exfiltration
â”‚   â””â”€â”€ Consent Violations
â”‚
â”œâ”€â”€ Discrimination & Bias
â”‚   â”œâ”€â”€ Race & Ethnicity
â”‚   â”œâ”€â”€ Gender & Sexual Orientation
â”‚   â”œâ”€â”€ Religion & Beliefs
â”‚   â”œâ”€â”€ Age
â”‚   â”œâ”€â”€ Disability
â”‚   â””â”€â”€ Socioeconomic Status
â”‚
â”œâ”€â”€ Manipulation & Coercion
â”‚   â”œâ”€â”€ Dark Patterns
â”‚   â”œâ”€â”€ Addiction Engineering
â”‚   â”œâ”€â”€ Emotional Manipulation
â”‚   â”œâ”€â”€ Gaslighting
â”‚   â””â”€â”€ Undue Influence
â”‚
â””â”€â”€ Exploitation
    â”œâ”€â”€ Child Exploitation
    â”œâ”€â”€ Labor Exploitation
    â”œâ”€â”€ Financial Exploitation
    â”œâ”€â”€ Vulnerable Populations
    â””â”€â”€ Power Imbalance Abuse
```

### Severity Levels

| Level | Description | Example | Response Time |
|-------|-------------|---------|---------------|
| **Critical** | Immediate harm, illegal activity | Child exploitation, suicide instructions | Immediate block + report |
| **High** | Significant harm potential | Hate speech, doxxing | Block within 1 hour |
| **Medium** | Moderate harm or policy violation | Misleading claims, bias | Review within 24 hours |
| **Low** | Minor policy violation | Borderline content | Review within 7 days |

### Category Definitions

#### 1. Harmful Content

**Definition**: Content that promotes, glorifies, or instructs on activities causing physical or psychological harm.

**Examples**:
- Violence: "Here's how to build an explosive device..."
- Self-harm: "Steps to harm yourself without being noticed..."
- Sexual: Non-consensual intimate content
- Hate: "All [group] should be eliminated..."

**Detection Approach**: Pattern matching, semantic analysis, context evaluation

---

#### 2. Deception & Misinformation

**Definition**: Intentionally false or misleading information designed to deceive.

**Examples**:
- Impersonation: "I am [celebrity], send money to..."
- Medical fraud: "This herb cures cancer, proven by studies..."
- Financial scam: "Guaranteed 500% returns in 24 hours..."

**Detection Approach**: Fact-checking, source verification, claim analysis

---

#### 3. Privacy Violations

**Definition**: Unauthorized collection, disclosure, or use of personal information.

**Examples**:
- PII leak: "John Doe, SSN: 123-45-6789, lives at..."
- Doxing: Publishing private address/phone without consent
- Surveillance: "Install this to track someone's location..."

**Detection Approach**: PII detection, context analysis, consent verification

---

#### 4. Discrimination & Bias

**Definition**: Unfair treatment or stereotyping based on protected characteristics.

**Examples**:
- Gender bias: "Women are naturally worse at [task]..."
- Racial discrimination: Job screening biased against certain ethnicities
- Age discrimination: "Don't hire anyone over 50..."

**Detection Approach**: Fairness metrics, demographic parity analysis, bias testing

---

#### 5. Manipulation & Coercion

**Definition**: Psychological tactics to influence behavior against user interests.

**Examples**:
- Dark patterns: Deceptive UI to trick users into subscriptions
- Addiction: Infinite scroll, variable rewards for engagement
- Emotional: "Act now or your family will suffer..."

**Detection Approach**: Pattern recognition, user flow analysis, psychological tactics database

---

#### 6. Exploitation

**Definition**: Taking unfair advantage of vulnerable individuals or power imbalances.

**Examples**:
- Child exploitation: Any form (strictly zero tolerance)
- Labor: Forced work, wage theft, unsafe conditions
- Financial: Elder fraud, predatory lending

**Detection Approach**: Age verification, vulnerability indicators, transaction patterns

---

## Dataset Versioning & Governance

### Dataset Structure

```
datasets/ethics/
â”œâ”€â”€ v1.0/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ harmful_content.jsonl        (10,000 samples)
â”‚   â”‚   â”œâ”€â”€ deception.jsonl              (8,000 samples)
â”‚   â”‚   â”œâ”€â”€ privacy_violations.jsonl     (6,000 samples)
â”‚   â”‚   â”œâ”€â”€ discrimination.jsonl         (12,000 samples)
â”‚   â”‚   â”œâ”€â”€ manipulation.jsonl           (5,000 samples)
â”‚   â”‚   â”œâ”€â”€ exploitation.jsonl           (3,000 samples)
â”‚   â”‚   â””â”€â”€ benign.jsonl                 (50,000 samples)
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ [same structure, 20% of train]
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â””â”€â”€ [same structure, held-out 15%]
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ CHANGELOG.md
â””â”€â”€ v1.1/
    â””â”€â”€ [updated samples + new categories]
```

### Sample Format

```json
{
  "id": "eth_001234",
  "text": "Sample agent action or user input text...",
  "category": "harmful_content",
  "subcategory": "violence",
  "severity": "high",
  "is_violation": true,
  "ground_truth_labels": ["violence", "weapon_instructions"],
  "context": {
    "platform": "chat",
    "user_demographics": {"age_group": "18-24", "region": "US"},
    "timestamp": "2024-11-15T10:30:00Z"
  },
  "annotator_metadata": {
    "annotator_id": "reviewer_042",
    "confidence": 0.95,
    "review_time_seconds": 45,
    "notes": "Clear violation, explicit weapon assembly instructions"
  },
  "version": "1.0",
  "created_at": "2024-10-01T00:00:00Z"
}
```

### Versioning Policy

**Semantic Versioning**: `MAJOR.MINOR.PATCH`

- **MAJOR**: Taxonomy changes, category additions/removals
- **MINOR**: New samples added (>1000), subcategory changes
- **PATCH**: Bug fixes, annotation corrections (<100 changes)

**Release Cadence**:
- Monthly: Patch releases (reviewer feedback integration)
- Quarterly: Minor releases (dataset expansion)
- Annually: Major releases (taxonomy revision)

### Quality Assurance

1. **Multi-Annotator Agreement**: Kappa coefficient â‰¥0.75 for release
2. **Expert Review**: 10% random sample reviewed by ethics board
3. **Bias Audit**: Demographic representation analysis
4. **Adversarial Testing**: Red team generates challenging edge cases

---

## Metrics & Targets

### Primary Metrics

| Category | Metric | Target | Critical Minimum | Rationale |
|----------|--------|--------|------------------|-----------|
| **Overall** | F1 Score | â‰¥0.92 | â‰¥0.90 | Balance precision & recall |
| **Overall** | Precision | â‰¥0.91 | â‰¥0.88 | Minimize false positives (user friction) |
| **Overall** | Recall | â‰¥0.90 | â‰¥0.88 | Minimize false negatives (harm prevention) |
| **Harmful Content** | Recall | â‰¥0.95 | â‰¥0.93 | Critical safety category |
| **Harmful Content** | FNR | â‰¤0.05 | â‰¤0.07 | False negatives = potential harm |
| **Privacy** | Precision | â‰¥0.96 | â‰¥0.94 | Avoid blocking legitimate PII usage |
| **Privacy** | Recall | â‰¥0.92 | â‰¥0.90 | Detect unauthorized disclosures |
| **Discrimination** | FNR | â‰¤0.08 | â‰¤0.10 | Prevent biased decisions |
| **Discrimination** | Fairness (DPD) | â‰¤0.10 | â‰¤0.15 | Demographic parity difference |
| **Manipulation** | Precision | â‰¥0.92 | â‰¥0.90 | Complex, context-dependent category |
| **Deception** | Recall | â‰¥0.90 | â‰¥0.88 | Catch misinformation campaigns |
| **Exploitation** | Recall | â‰¥0.98 | â‰¥0.95 | Zero tolerance for CSAM, severe exploitation |

### Metric Definitions

**Precision**: True Positives / (True Positives + False Positives)
- Interpretation: Of flagged items, what % are actual violations?
- Impact: High FP = user frustration, over-blocking

**Recall (Sensitivity)**: True Positives / (True Positives + False Negatives)
- Interpretation: Of actual violations, what % are detected?
- Impact: Low recall = harm slips through

**F1 Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- Interpretation: Harmonic mean balancing precision & recall
- Impact: Overall detection quality

**False Negative Rate (FNR)**: False Negatives / (True Positives + False Negatives) = 1 - Recall
- Interpretation: What % of violations are missed?
- Impact: Critical for safety categories

**Demographic Parity Difference (DPD)**: |P(Å·=1|A=a) - P(Å·=1|A=b)|
- Interpretation: Difference in positive prediction rates across demographic groups
- Impact: Fairness across protected attributes

### Secondary Metrics

- **AUC-ROC**: Area under ROC curve (threshold-independent performance)
- **AUC-PR**: Area under Precision-Recall curve (imbalanced classes)
- **Calibration Error (ECE)**: Expected Calibration Error for confidence scores
- **Latency**: p95 inference time <100ms, p99 <200ms
- **Throughput**: Evaluations per second under load

### Per-Category Targets

Based on `Validation_plan.md` thresholds:

```yaml
# config/ethics_targets.yaml
targets:
  overall:
    f1: 0.92
    precision: 0.91
    recall: 0.90
  
  harmful_content:
    recall: 0.95
    fnr: 0.05
    precision: 0.90
  
  privacy_violations:
    precision: 0.96
    recall: 0.92
  
  discrimination:
    fnr: 0.08
    fairness_dpd: 0.10
  
  deception:
    recall: 0.90
    precision: 0.88
  
  manipulation:
    f1: 0.90
    precision: 0.92
  
  exploitation:
    recall: 0.98  # Near-perfect recall required
    precision: 0.95
```

---

## Threshold Tuning Strategy

### Objective

Optimize detection thresholds to maximize F1 score while satisfying per-category constraints (e.g., FNR <0.07 for harmful content).

### Approach: Multi-Objective Optimization

**Stage 1: Grid Search** (Coarse)

```python
# scripts/ethics/tune_thresholds.py
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

def grid_search_thresholds(y_true, y_scores, category_constraints):
    """
    Find optimal threshold per category using grid search.
    
    Args:
        y_true: Ground truth labels (N x C) one-hot encoded
        y_scores: Model scores (N x C) for each category
        category_constraints: Dict of {category: {metric: threshold}}
    
    Returns:
        optimal_thresholds: Dict of {category: threshold}
    """
    optimal_thresholds = {}
    
    for cat_idx, category in enumerate(CATEGORIES):
        best_f1 = 0
        best_threshold = 0.5
        
        # Grid search from 0.1 to 0.9 in steps of 0.05
        for threshold in np.arange(0.1, 0.95, 0.05):
            y_pred = (y_scores[:, cat_idx] >= threshold).astype(int)
            
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_true[:, cat_idx], y_pred, average='binary'
            )
            
            fnr = 1 - recall
            
            # Check constraints
            constraints = category_constraints.get(category, {})
            if fnr > constraints.get('max_fnr', 1.0):
                continue  # Violates FNR constraint
            if precision < constraints.get('min_precision', 0.0):
                continue  # Violates precision constraint
            
            # Update if better F1
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        optimal_thresholds[category] = best_threshold
        print(f"{category}: threshold={best_threshold:.3f}, F1={best_f1:.3f}")
    
    return optimal_thresholds
```

**Stage 2: Bayesian Optimization** (Fine)

```python
from skopt import gp_minimize
from skopt.space import Real
from sklearn.metrics import f1_score

def objective_function(thresholds, y_true, y_scores, constraints):
    """Objective: maximize F1 subject to constraints"""
    y_pred = np.zeros_like(y_true)
    
    for cat_idx, threshold in enumerate(thresholds):
        y_pred[:, cat_idx] = (y_scores[:, cat_idx] >= threshold).astype(int)
    
    # Compute per-category metrics
    f1_scores = []
    penalty = 0
    
    for cat_idx, category in enumerate(CATEGORIES):
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true[:, cat_idx], y_pred[:, cat_idx], average='binary'
        )
        f1_scores.append(f1)
        
        # Penalty for constraint violations
        fnr = 1 - recall
        if fnr > constraints[category].get('max_fnr', 1.0):
            penalty += (fnr - constraints[category]['max_fnr']) * 10
    
    # Weighted average F1 - penalty
    overall_f1 = np.mean(f1_scores) - penalty
    
    return -overall_f1  # Minimize negative F1

# Run Bayesian optimization
space = [Real(0.1, 0.9, name=f'threshold_{cat}') for cat in CATEGORIES]

result = gp_minimize(
    lambda x: objective_function(x, y_true_val, y_scores_val, constraints),
    space,
    n_calls=200,
    random_state=42
)

optimal_thresholds = dict(zip(CATEGORIES, result.x))
```

### Tuning Workflow

```bash
# 1. Collect validation predictions
python scripts/ethics/evaluate.py \
  --dataset datasets/ethics/v1.0/validation/ \
  --output predictions_val.json

# 2. Run threshold tuning
python scripts/ethics/tune_thresholds.py \
  --predictions predictions_val.json \
  --constraints config/ethics_targets.yaml \
  --method bayesian \
  --output thresholds_optimized.json

# 3. Validate on test set
python scripts/ethics/evaluate.py \
  --dataset datasets/ethics/v1.0/test/ \
  --thresholds thresholds_optimized.json \
  --output test_results.json

# 4. Generate report
python scripts/ethics/generate_report.py \
  --results test_results.json \
  --output docs/ethics_report.md
```

### Threshold Versioning

Thresholds are versioned alongside datasets:

```yaml
# config/thresholds_v1.0.yaml
version: "1.0"
dataset_version: "1.0"
tuned_date: "2024-11-15"
method: "bayesian"

thresholds:
  harmful_content: 0.42
  deception: 0.55
  privacy_violations: 0.48
  discrimination: 0.52
  manipulation: 0.60
  exploitation: 0.35  # Lower threshold = higher recall

validation_metrics:
  overall_f1: 0.923
  harmful_content_recall: 0.954
  privacy_precision: 0.962
```

---

## Drift Monitoring

### Objective

Detect degradation in detection performance due to:
- Data distribution shift (new attack patterns)
- Model degradation over time
- Threshold miscalibration

### Statistical Tests

#### 1. Population Stability Index (PSI)

**Purpose**: Measure distribution shift in model scores.

**Formula**:
```
PSI = Î£ (actual_% - expected_%) Ã— ln(actual_% / expected_%)
```

**Interpretation**:
- PSI < 0.1: No significant shift
- 0.1 â‰¤ PSI < 0.2: Moderate shift (investigate)
- PSI â‰¥ 0.2: Significant shift (recalibrate)

**Implementation**:

```python
# scripts/ethics/monitor_drift.py
import numpy as np

def calculate_psi(expected_scores, actual_scores, bins=10):
    """Calculate Population Stability Index"""
    # Create score bins
    score_bins = np.linspace(0, 1, bins + 1)
    
    # Calculate distributions
    expected_dist, _ = np.histogram(expected_scores, bins=score_bins)
    actual_dist, _ = np.histogram(actual_scores, bins=score_bins)
    
    # Normalize to percentages
    expected_pct = (expected_dist / len(expected_scores)) + 1e-10
    actual_pct = (actual_dist / len(actual_scores)) + 1e-10
    
    # Calculate PSI
    psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))
    
    return psi

# Monitor daily
psi_daily = calculate_psi(baseline_scores, today_scores)
if psi_daily > 0.2:
    alert("Significant drift detected", severity="high")
```

#### 2. Kolmogorov-Smirnov Test

**Purpose**: Test if two score distributions are identical.

**Null Hypothesis**: Baseline and current distributions are from same distribution.

**Implementation**:

```python
from scipy.stats import ks_2samp

def ks_drift_test(baseline_scores, current_scores, alpha=0.05):
    """
    Perform KS test for distribution drift.
    
    Returns:
        drift_detected: bool
        statistic: float (distance between CDFs)
        p_value: float
    """
    statistic, p_value = ks_2samp(baseline_scores, current_scores)
    
    drift_detected = p_value < alpha
    
    return drift_detected, statistic, p_value

# Run weekly
drift, ks_stat, p_val = ks_drift_test(baseline_scores, week_scores)
if drift:
    alert(f"KS test: drift detected (stat={ks_stat:.3f}, p={p_val:.4f})")
```

### Monitoring Workflow

```yaml
# Drift monitoring configuration
drift_monitoring:
  cadence:
    psi: daily
    ks_test: weekly
    metrics: daily
  
  thresholds:
    psi_warning: 0.1
    psi_critical: 0.2
    ks_alpha: 0.05
    metric_degradation: 0.05  # 5% drop from baseline
  
  baseline:
    dataset: "datasets/ethics/v1.0/test/"
    scores_file: "baseline_scores_v1.0.npy"
    metrics_file: "baseline_metrics_v1.0.json"
  
  alerts:
    - condition: "psi > 0.2"
      action: "create_issue"
      severity: "high"
      template: "ethics_recalibration"
    
    - condition: "f1_score < baseline_f1 - 0.05"
      action: "create_issue"
      severity: "critical"
      template: "ethics_degradation"
```

### Drift Response

**Trigger**: PSI >0.2 or metric degradation >5%

**Actions**:
1. Auto-create GitHub issue: "Ethics Model Recalibration Required"
2. Run diagnostic analysis:
   - Identify shifted categories
   - Analyze failure patterns
   - Review recent false positives/negatives
3. Freeze model deployment (optional, for critical drift)
4. Initiate recalibration workflow (see [Recalibration Workflow](#recalibration-workflow))

---

## Reviewer Feedback Integration

### Human Review Loop

**Workflow**:

```
Detection â†’ Confidence Score â†’ Route Decision
                                  â”œâ”€ High Confidence (>0.9) â†’ Auto-Action
                                  â”œâ”€ Medium (0.6-0.9) â†’ Async Review Queue
                                  â””â”€ Low (<0.6) â†’ Priority Review
                                          â†“
                                     Human Reviewer
                                          â†“
                                   Reviewer Feedback
                                    (Agree/Disagree)
                                          â†“
                                   Feedback Database
                                          â†“
                              Monthly Dataset Update
                                          â†“
                                  Model Retraining
```

### Feedback Schema

```json
{
  "feedback_id": "fb_789012",
  "decision_id": "dec_456789",
  "original_detection": {
    "category": "discrimination",
    "severity": "medium",
    "confidence": 0.72,
    "model_version": "v2.1.3"
  },
  "reviewer_assessment": {
    "reviewer_id": "reviewer_015",
    "agrees_with_detection": false,
    "correct_category": "benign",
    "correct_severity": null,
    "reasoning": "Context indicates legitimate discussion of diversity, not discriminatory intent",
    "confidence": 0.90,
    "review_timestamp": "2024-11-20T14:30:00Z"
  },
  "metadata": {
    "review_time_seconds": 120,
    "difficult_case": true,
    "tags": ["context_dependent", "edge_case"]
  }
}
```

### Feedback Processing Pipeline

```python
# scripts/ethics/process_feedback.py

def process_monthly_feedback(feedback_file, output_dir):
    """
    Process reviewer feedback and prepare dataset updates.
    
    Steps:
    1. Load feedback records
    2. Filter high-confidence disagreements
    3. Re-annotate samples
    4. Add to dataset update queue
    5. Calculate inter-rater agreement
    """
    feedback_df = pd.read_json(feedback_file, lines=True)
    
    # Filter for actionable feedback
    actionable = feedback_df[
        (feedback_df['reviewer_assessment.confidence'] > 0.85) &
        (feedback_df['reviewer_assessment.agrees_with_detection'] == False)
    ]
    
    # Calculate agreement statistics
    agreement_rate = (
        feedback_df['reviewer_assessment.agrees_with_detection'].sum() / 
        len(feedback_df)
    )
    
    print(f"Model-Reviewer Agreement: {agreement_rate:.2%}")
    
    # Create updated samples
    updated_samples = []
    for _, row in actionable.iterrows():
        sample = {
            'id': f"fb_update_{row['feedback_id']}",
            'text': fetch_original_text(row['decision_id']),
            'category': row['reviewer_assessment.correct_category'],
            'severity': row['reviewer_assessment.correct_severity'],
            'is_violation': row['reviewer_assessment.correct_category'] != 'benign',
            'source': 'reviewer_correction',
            'original_model_prediction': row['original_detection.category'],
            'correction_reason': row['reviewer_assessment.reasoning']
        }
        updated_samples.append(sample)
    
    # Write to update file
    output_path = output_dir / f"feedback_updates_{datetime.now():%Y%m}.jsonl"
    with open(output_path, 'w') as f:
        for sample in updated_samples:
            f.write(json.dumps(sample) + '\n')
    
    return {
        'total_feedback': len(feedback_df),
        'actionable_corrections': len(updated_samples),
        'agreement_rate': agreement_rate
    }
```

### Reviewer Calibration

**Objective**: Ensure consistent annotation quality across reviewers.

**Process**:
1. **Onboarding**: New reviewers annotate 100 gold-standard samples
   - Agreement with gold standard >90% required
2. **Ongoing Calibration**: Monthly review of 50 common samples
   - Inter-rater agreement (Fleiss' Kappa) >0.75 target
3. **Drift Detection**: Monitor per-reviewer agreement trends
   - Flagstray outliers for retraining

**Implementation**:

```python
# scripts/ethics/reviewer_calibration.py
from sklearn.metrics import cohen_kappa_score

def calculate_inter_rater_agreement(annotations_df):
    """
    Calculate Fleiss' Kappa for multi-rater agreement.
    
    annotations_df: columns = [sample_id, reviewer_id, category]
    """
    # Pivot to matrix: rows=samples, cols=reviewers
    matrix = annotations_df.pivot(
        index='sample_id', 
        columns='reviewer_id', 
        values='category'
    )
    
    # Calculate Fleiss' Kappa (simplified for binary)
    # Full implementation would use statsmodels.stats.inter_rater
    kappa = fleiss_kappa(matrix)
    
    return kappa

# Monthly calibration
calibration_results = calculate_inter_rater_agreement(monthly_annotations)
if calibration_results < 0.75:
    alert("Reviewer calibration below target", severity="medium")
```

---

## Reporting & Artifacts

### Ethics Report Format

**File**: `docs/ethics_report_YYYY-MM.md`

**Structure**:

```markdown
# Nethical Ethics Validation Report
**Period**: November 2024  
**Dataset Version**: v1.1  
**Model Version**: v2.3.1  
**Report Generated**: 2024-11-24

## Executive Summary
- Overall F1 Score: 0.925 (Target: â‰¥0.92) âœ…
- Critical Metrics Met: 6/6 âœ…
- Drift Detected: None
- Reviewer Feedback: 1,234 cases reviewed, 91% agreement

## Performance by Category

### Harmful Content
- **Recall**: 0.956 (Target: â‰¥0.95) âœ…
- **Precision**: 0.912
- **F1**: 0.933
- **FNR**: 0.044 (Target: â‰¤0.05) âœ…
- **Sample Size**: 2,000 test cases

**Analysis**: Performance exceeds targets. False negatives primarily in ambiguous satire/parody cases (22 of 88 FNs).

### Privacy Violations
- **Precision**: 0.964 (Target: â‰¥0.96) âœ…
- **Recall**: 0.928 (Target: â‰¥0.92) âœ…
- **F1**: 0.946

**Analysis**: Strong performance. 6 false positives due to legitimate data sharing in enterprise contexts.

[... other categories ...]

## Drift Analysis
- **PSI (7-day)**: 0.08 (No significant shift)
- **KS Test (weekly)**: p=0.12 (No significant drift)
- **Metric Stability**: F1 variance <0.01 over past 30 days

## Fairness Analysis

### Demographic Parity

| Protected Attribute | Group A | Group B | DPD | Status |
|---------------------|---------|---------|-----|--------|
| Gender | M: 0.15 | F: 0.14 | 0.01 | âœ… |
| Age | 18-30: 0.16 | 50+: 0.14 | 0.02 | âœ… |
| Region | US: 0.15 | EU: 0.16 | 0.01 | âœ… |

**Conclusion**: No significant bias detected across demographic groups.

## Reviewer Feedback Integration
- **Total Reviews**: 1,234
- **Agreement with Model**: 91%
- **Corrections Applied**: 108 samples added to v1.2 dataset
- **Common Disagreement Patterns**:
  1. Context-dependent manipulation (34%)
  2. Satirical harmful content (28%)
  3. Regional cultural differences (18%)

## Recommendations
1. âœ… Continue current model version (no degradation)
2. ðŸ”„ Integrate 108 corrected samples into v1.2 dataset (scheduled Dec 2024)
3. ðŸ”„ Develop context-aware manipulation detector (Q1 2025)
4. âœ… Maintain current monitoring cadence

## Artifacts
- Test results: `artifacts/ethics_test_results_2024-11.json`
- Confusion matrices: `artifacts/confusion_matrices_2024-11.png`
- ROC curves: `artifacts/roc_curves_2024-11.png`
- Threshold configuration: `config/thresholds_v1.1.yaml`

---
**Approved by**: Ethics Review Board  
**Next Review**: December 2024
```

### Automated Report Generation

```bash
# Monthly cron job
0 0 1 * * /opt/nethical/scripts/ethics/generate_monthly_report.sh
```

```bash
#!/bin/bash
# scripts/ethics/generate_monthly_report.sh

set -euo pipefail

MONTH=$(date +%Y-%m)
REPORT_FILE="docs/ethics_report_${MONTH}.md"

echo "Generating ethics report for $MONTH..."

# 1. Run full evaluation on test set
python scripts/ethics/evaluate.py \
  --dataset datasets/ethics/v1.1/test/ \
  --output artifacts/ethics_test_results_${MONTH}.json

# 2. Calculate fairness metrics
python scripts/ethics/fairness_analysis.py \
  --results artifacts/ethics_test_results_${MONTH}.json \
  --output artifacts/fairness_metrics_${MONTH}.json

# 3. Analyze drift
python scripts/ethics/monitor_drift.py \
  --baseline artifacts/baseline_scores_v1.1.npy \
  --current artifacts/current_scores_${MONTH}.npy \
  --output artifacts/drift_analysis_${MONTH}.json

# 4. Process reviewer feedback
python scripts/ethics/process_feedback.py \
  --feedback data/reviewer_feedback_${MONTH}.jsonl \
  --output datasets/ethics/updates/

# 5. Generate report
python scripts/ethics/generate_report.py \
  --results artifacts/ethics_test_results_${MONTH}.json \
  --fairness artifacts/fairness_metrics_${MONTH}.json \
  --drift artifacts/drift_analysis_${MONTH}.json \
  --feedback datasets/ethics/updates/ \
  --output "$REPORT_FILE"

echo "Report generated: $REPORT_FILE"

# 6. Commit report to repository
git add "$REPORT_FILE" artifacts/
git commit -m "Add ethics validation report for $MONTH"
git push
```

---

## Recalibration Workflow

### Trigger Conditions

1. **Drift Alert**: PSI >0.2 or KS test p<0.05
2. **Metric Degradation**: F1 drop >5% or critical FNR threshold breach
3. **Feedback Accumulation**: >500 high-confidence disagreements
4. **Scheduled**: Quarterly proactive recalibration

### Recalibration Process

```
1. Issue Creation (Automated)
   â”œâ”€ Title: "Ethics Model Recalibration Required - [Reason]"
   â”œâ”€ Labels: ethics, recalibration, priority:high
   â””â”€ Assignees: @ethics-team, @ml-ops

2. Root Cause Analysis
   â”œâ”€ Identify shifted categories
   â”œâ”€ Analyze failure patterns
   â””â”€ Review recent adversarial examples

3. Dataset Update (if needed)
   â”œâ”€ Integrate reviewer feedback
   â”œâ”€ Add adversarial samples
   â”œâ”€ Balance class distribution
   â””â”€ Version bump (v1.X â†’ v1.Y)

4. Threshold Retuning
   â”œâ”€ Run Bayesian optimization
   â”œâ”€ Validate on hold-out set
   â””â”€ Update config/thresholds_vX.Y.yaml

5. Model Retraining (if needed)
   â”œâ”€ Retrain on updated dataset
   â”œâ”€ Hyperparameter tuning
   â”œâ”€ Cross-validation
   â””â”€ Version bump (vA.B â†’ vA.C)

6. Validation
   â”œâ”€ Full benchmark on test set
   â”œâ”€ Fairness analysis
   â”œâ”€ A/B test in staging (7 days)
   â””â”€ Ethics board review

7. Deployment
   â”œâ”€ Canary release (5% traffic, 24h)
   â”œâ”€ Progressive rollout (25% â†’ 50% â†’ 100%)
   â””â”€ Monitor for regressions

8. Documentation
   â”œâ”€ Update CHANGELOG.md
   â”œâ”€ Generate recalibration report
   â””â”€ Close recalibration issue
```

### Recalibration SLA

| Severity | Response Time | Completion Target |
|----------|---------------|-------------------|
| **Critical** (FNR breach in exploitation) | <4 hours | <24 hours |
| **High** (Major drift, metric drop >10%) | <24 hours | <7 days |
| **Medium** (Moderate drift) | <72 hours | <30 days |
| **Low** (Scheduled quarterly) | Planned | 2 weeks |

---

## Risk of Regression & Mitigations

### Regression Risks

1. **New Dataset Bias**: Updated dataset introduces new biases
   - **Mitigation**: Mandatory fairness analysis before release
   
2. **Threshold Miscalibration**: Aggressive tuning causes overfitting
   - **Mitigation**: Hold-out validation set, A/B testing
   
3. **Model Degradation**: Retraining degrades performance on original categories
   - **Mitigation**: Benchmark on multiple dataset versions
   
4. **Adversarial Overfitting**: Model memorizes adversarial examples
   - **Mitigation**: Diverse adversarial generation, regularization

### Regression Testing

```python
# scripts/ethics/regression_test.py

def regression_test_suite(old_model, new_model, test_datasets):
    """
    Comprehensive regression testing.
    
    Tests:
    1. Performance on all historical dataset versions
    2. Fairness metrics consistency
    3. Latency regression
    4. Known adversarial examples
    """
    results = {}
    
    # Test 1: Historical datasets
    for dataset_version, dataset in test_datasets.items():
        old_metrics = evaluate_model(old_model, dataset)
        new_metrics = evaluate_model(new_model, dataset)
        
        # Check for degradation
        for metric in ['f1', 'recall', 'precision']:
            delta = new_metrics[metric] - old_metrics[metric]
            if delta < -0.02:  # >2% degradation
                results[f"{dataset_version}_{metric}"] = "FAIL"
            else:
                results[f"{dataset_version}_{metric}"] = "PASS"
    
    # Test 2: Fairness
    old_fairness = compute_fairness(old_model, demographic_data)
    new_fairness = compute_fairness(new_model, demographic_data)
    
    if new_fairness['max_dpd'] > old_fairness['max_dpd'] * 1.2:
        results['fairness'] = "FAIL"
    else:
        results['fairness'] = "PASS"
    
    # Test 3: Latency
    old_latency = benchmark_latency(old_model)
    new_latency = benchmark_latency(new_model)
    
    if new_latency['p95'] > old_latency['p95'] * 1.1:  # >10% slower
        results['latency'] = "FAIL"
    else:
        results['latency'] = "PASS"
    
    return results

# Run before deployment
regression_results = regression_test_suite(
    old_model='models/ethics_v2.3.1',
    new_model='models/ethics_v2.4.0',
    test_datasets=load_all_test_datasets()
)

if "FAIL" in regression_results.values():
    raise ValueError("Regression test failed", regression_results)
```

### Rollback Plan

**Trigger**: Critical regression detected post-deployment (F1 <0.85 or FNR >0.15)

**Process**:
1. Immediate rollback to previous model version (automated)
2. Incident report creation
3. Post-mortem analysis
4. Root cause remediation
5. Re-validation before retry

---

## Document Relationships

This ethics validation framework integrates with:

- **[Validation Plan](./Validation_plan.md)**: References ethics benchmark thresholds and cadence (weekly mini, monthly full)
- **[Production Readiness Checklist](./PRODUCTION_READINESS_CHECKLIST.md)**: Ethics & safety section validates against this framework
- **[Security Hardening Guide](./SECURITY_HARDENING_GUIDE.md)**: Audit integrity controls support dataset governance
- **[Benchmark Plan](./BENCHMARK_PLAN.md)**: Ethics evaluation latency measured in performance benchmarks

---

**Version**: 1.0  
**Last Updated**: 2025-11-24  
**Status**: Comprehensive framework aligned with validation plan metrics  
**Next Review**: Monthly (automated report generation)
