name: ML Training Pipeline

on:
  workflow_dispatch:
    inputs: 
      model_types:
        description: 'Model types to train (comma-separated or "all")'
        required: false
        default: 'all'
      force_retrain: 
        description: 'Force retrain even if recent models exist'
        required: false
        type: boolean
        default: false
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2 AM UTC
  push:
    paths:
      - 'training/**'
      - 'nethical/mlops/**'
      - 'datasets/datasets'
    branches:
      - main

permissions:
  contents: write
  issues: write

jobs:
  train-models:
    name: Train ML Models
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          pip install --upgrade pip setuptools wheel

      - name: Install Python dependencies
        run: |
          pip install numpy pandas kaggle pyyaml

      - name:  Install Nethical package
        run: |
          pip install -e .

      - name: Verify installation
        run: |
          python -c "import nethical; print('Nethical package installed successfully')"
          python -c "import yaml; print('PyYAML installed successfully')"
          python -c "from nethical.core.audit_merkle import MerkleAnchor; print('MerkleAnchor available')" || echo "MerkleAnchor not available (optional)"
          python -c "from nethical.core.ethical_drift_reporter import EthicalDriftReporter; print('EthicalDriftReporter available')" || echo "EthicalDriftReporter not available (optional)"
          python -c "from nethical.core.governance import EnhancedSafetyGovernance; print('Governance available')" || echo "Governance not available (optional)"
          python -c "from nethical. mlops.adversarial import AdversarialGenerator; print('AdversarialGenerator available')" || echo "AdversarialGenerator not available (optional)"

      - name: Create output directories
        run: |
          mkdir -p models/candidates models/current models/metadata
          mkdir -p training_drift_reports
          mkdir -p training_audit_logs

      - name: Run training script
        id: train
        run: |
          FORCE_FLAG=""
          if [ "${{ github.event. inputs.force_retrain }}" = "true" ]; then
            FORCE_FLAG="--force-retrain"
          fi
          
          python training/train_any_model.py \
            --model-type ${{ github.event.inputs.model_types || 'all' }} \
            --epochs 30 \
            --batch-size 64 \
            --num-samples 20000 \
            --include-adversarial \
            --enable-governance \
            --enable-drift-tracking \
            --enable-audit \
            --no-download \
            $FORCE_FLAG
          
          # Parse training summary to extract outputs
          if [ -f "training_audit_logs/training_summary.json" ]; then
            TRAINED=$(jq -r '.total_models // 0' training_audit_logs/training_summary.json)
            SUCCESS=$(jq -r '. successful // 0' training_audit_logs/training_summary.json)
            PROMOTED=$(jq -r '.promoted // 0' training_audit_logs/training_summary.json)
            echo "models-trained=${TRAINED}" >> $GITHUB_OUTPUT
            echo "training-success=${SUCCESS}" >> $GITHUB_OUTPUT
            echo "models-promoted=${PROMOTED}" >> $GITHUB_OUTPUT
          fi

      - name: Upload baseline models
        uses: actions/upload-artifact@v4
        with:
          name: baseline-models
          path: models/current/
          retention-days: 30
          if-no-files-found: warn
        if: always()

      - name: Upload drift reports
        uses: actions/upload-artifact@v4
        with: 
          name: drift-reports-${{ github.run_number }}
          path: training_drift_reports/
          retention-days: 30
          if-no-files-found: warn
        if: always()

      - name: Upload audit logs
        uses:  actions/upload-artifact@v4
        with:
          name:  audit-logs-${{ github. run_number }}
          path:  training_audit_logs/
          retention-days: 90
          if-no-files-found: warn
        if: always()

      - name: Upload all trained models
        uses: actions/upload-artifact@v4
        with:
          name: all-trained-models-${{ github.run_number }}
          path: |
            models/current/
            models/candidates/
          retention-days: 30
          if-no-files-found:  warn
        if: always()

  track-performance:
    name: Track Model Performance
    needs: train-models
    runs-on:  ubuntu-latest
    if: success()
    permissions:
      contents: write
      issues: write

    steps: 
      - uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install numpy pandas pyyaml

      - name: Download baseline models
        uses: actions/download-artifact@v4
        with: 
          name: baseline-models
          path: models/baseline
        continue-on-error: true

      - name: Download performance history
        uses: actions/download-artifact@v4
        with: 
          name: performance-history
          path: performance_history/
        continue-on-error: true

      - name: Compare with previous models
        id: compare
        run: |
          echo "Performance tracking placeholder"
          echo "This step will compare new models with baseline"
          
          # Create performance history entry
          mkdir -p performance_history
          TIMESTAMP=$(date -u +"%Y%m%d_%H%M%S")
          echo "{\"timestamp\": \"$TIMESTAMP\", \"run_id\": \"${{ github.run_id }}\"}" > performance_history/run_${TIMESTAMP}.json

      - name: Upload performance history
        uses: actions/upload-artifact@v4
        with: 
          name: performance-history
          path: performance_history/
          retention-days: 365
        if: always()

      - name: Create performance report
        if: always()
        run: |
          echo "## ðŸ“Š Training Performance Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "- Models Trained: ${{ needs.train-models.outputs.models-trained || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Training Success: ${{ needs.train-models.outputs.training-success || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Models Promoted: ${{ needs.train-models.outputs.models-promoted || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
