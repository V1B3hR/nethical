name: Performance Regression Detection

on:
  pull_request:
    branches:
      - main
      - develop
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      baseline_ref:
        description: 'Baseline reference (branch/tag/commit) to compare against'
        required: false
        default: 'main'

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  performance-benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for comparison

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Run load test
        run: |
          if [ "$CI" = "true" ]; then
            python examples/perf/generate_load.py --agents 50 --rps 15 --duration 60
          else
            python examples/perf/generate_load.py --agents 100 --rps 50 --duration 60
          fi

      - name: Run current branch benchmarks
        id: current_bench
        run: |
          echo "Running benchmarks on current branch..."
          if [ -f "examples/perf/generate_load.py" ]; then
            python examples/perf/generate_load.py \
              --agents 100 \
              --rps 50 \
              --duration 30 \
              --cohort benchmark \
              --output perf_results_current.csv
          fi

          if [ -f "perf_results_current.csv" ]; then
            # Compute true P50/P95 in Python, assuming latency in column 3 (0-based index 2)
            python3 - << 'PY' > current_metrics.out
import csv, math
lat = []
with open('perf_results_current.csv', newline='') as f:
    r = csv.reader(f)
    next(r, None)  # skip header
    for row in r:
        try:
            lat.append(float(row[2]))
        except Exception:
            pass
lat.sort()
def percentile(data, p):
    if not data: return None
    k = (len(data) - 1) * (p / 100)
    f = math.floor(k); c = math.ceil(k)
    if f == c: return data[int(k)]
    return data[f] * (c - k) + data[c] * (k - f)
p50 = percentile(lat, 50)
p95 = percentile(lat, 95)
if p50 is not None: print(f"current_p50={p50:.2f}")
if p95 is not None: print(f"current_p95={p95:.2f}")
PY
            cat current_metrics.out
            cat current_metrics.out >> "$GITHUB_OUTPUT"
          else
            echo "No perf_results_current.csv found; skipping metrics extraction."
          fi

      - name: Checkout baseline
        if: github.event_name == 'pull_request'
        run: |
          git fetch origin ${{ github.base_ref }}
          git checkout origin/${{ github.base_ref }}

      - name: Run baseline benchmarks
        if: github.event_name == 'pull_request'
        id: baseline_bench
        run: |
          echo "Running benchmarks on baseline..."

          # Reinstall dependencies for baseline (ensure base branch requirements are applied)
          pip install -r requirements.txt
          pip install -e .

          if [ -f "examples/perf/generate_load.py" ]; then
            python examples/perf/generate_load.py \
              --agents 100 \
              --rps 50 \
              --duration 30 \
              --cohort benchmark \
              --output perf_results_baseline.csv
          fi

          if [ -f "perf_results_baseline.csv" ]; then
            python3 - << 'PY' > baseline_metrics.out
import csv, math
lat = []
with open('perf_results_baseline.csv', newline='') as f:
    r = csv.reader(f)
    next(r, None)  # skip header
    for row in r:
        try:
            lat.append(float(row[2]))
        except Exception:
            pass
lat.sort()
def percentile(data, p):
    if not data: return None
    k = (len(data) - 1) * (p / 100)
    f = math.floor(k); c = math.ceil(k)
    if f == c: return data[int(k)]
    return data[f] * (c - k) + data[c] * (k - f)
p50 = percentile(lat, 50)
p95 = percentile(lat, 95)
if p50 is not None: print(f"baseline_p50={p50:.2f}")
if p95 is not None: print(f"baseline_p95={p95:.2f}")
PY
            cat baseline_metrics.out
            cat baseline_metrics.out >> "$GITHUB_OUTPUT"
          else
            echo "No perf_results_baseline.csv found; skipping metrics extraction."
          fi

      - name: Compare performance
        if: github.event_name == 'pull_request'
        id: compare
        env:
          CURRENT_P50: ${{ steps.current_bench.outputs.current_p50 }}
          CURRENT_P95: ${{ steps.current_bench.outputs.current_p95 }}
          BASELINE_P50: ${{ steps.baseline_bench.outputs.baseline_p50 }}
          BASELINE_P95: ${{ steps.baseline_bench.outputs.baseline_p95 }}
        run: |
          python3 - << 'PY' > compare.out
import os
def to_float(v):
    try:
        return float(v)
    except (TypeError, ValueError):
        return None

cp50 = to_float(os.getenv('CURRENT_P50'))
cp95 = to_float(os.getenv('CURRENT_P95'))
bp50 = to_float(os.getenv('BASELINE_P50'))
bp95 = to_float(os.getenv('BASELINE_P95'))

def pct_change(cur, base):
    if cur is None or base in (None, 0.0):
        return "N/A"
    return f"{((cur - base) / base) * 100:.2f}"

p50_change = pct_change(cp50, bp50)
p95_change = pct_change(cp95, bp95)

print(f"p50_change={p50_change}")
print(f"p95_change={p95_change}")

# Only flag regression if we have numeric p50_change
if p50_change != "N/A":
    regression = float(p50_change) > 10.0
    print(f"regression_detected={'true' if regression else 'false'}")
else:
    print("regression_detected=false")
PY
          cat compare.out
          cat compare.out >> "$GITHUB_OUTPUT"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            perf_results_*.csv
        if: always()

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const currentP50 = '${{ steps.current_bench.outputs.current_p50 }}';
            const currentP95 = '${{ steps.current_bench.outputs.current_p95 }}';
            const baselineP50 = '${{ steps.baseline_bench.outputs.baseline_p50 }}';
            const baselineP95 = '${{ steps.baseline_bench.outputs.baseline_p95 }}';
            const p50Change = '${{ steps.compare.outputs.p50_change }}';
            const p95Change = '${{ steps.compare.outputs.p95_change }}';
            const regressionDetected = '${{ steps.compare.outputs.regression_detected }}' === 'true';

            let emoji = regressionDetected ? '⚠️' : '✅';
            let status = regressionDetected ? '**Performance Regression Detected**' : 'No Performance Regression';

            let comment = `## ${emoji} Performance Benchmark Results\n\n`;
            comment += `### ${status}\n\n`;
            comment += '| Metric | Baseline | Current | Change |\n';
            comment += '|--------|----------|---------|--------|\n';
            comment += `| P50 Latency | ${baselineP50 || 'N/A'} ms | ${currentP50 || 'N/A'} ms | ${p50Change || 'N/A'}% |\n`;
            comment += `| P95 Latency | ${baselineP95 || 'N/A'} ms | ${currentP95 || 'N/A'} ms | ${p95Change || 'N/A'}% |\n`;
            comment += '\n';

            if (regressionDetected) {
              comment += '⚠️ **Warning**: P50 latency has increased by more than 10%.\n';
              comment += 'Please review the changes for potential performance impacts.\n\n';
            } else {
              comment += '✅ Performance is within acceptable limits.\n\n';
            }

            comment += '---\n';
            comment += '*Benchmark Configuration: 100 agents, 50 RPS, 30 seconds*\n';
            comment += `*Baseline: \`${context.payload.pull_request.base.ref}\`*\n`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
        continue-on-error: true

      - name: Fail on significant regression
        if: github.event_name == 'pull_request' && steps.compare.outputs.regression_detected == 'true'
        run: |
          echo "::warning::Performance regression detected (>10% increase in P50 latency)"
          echo "Please review the changes and optimize if necessary"
          # Don't fail the build, just warn
          exit 0

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install memory-profiler psutil

      - name: Run memory profiling
        run: |
          echo "Running memory profiling..."

          # Create a simple memory profiling script
          cat > memory_test.py << 'EOF'
          from nethical.core import IntegratedGovernance
          from memory_profiler import profile
          import psutil
          import os

          @profile
          def test_governance_memory():
              process = psutil.Process(os.getpid())
              start_memory = process.memory_info().rss / 1024 / 1024  # MB

              gov = IntegratedGovernance(
                  storage_dir="./test_data",
                  enable_performance_optimization=True,
                  enable_merkle_anchoring=True,
                  enable_quarantine=True
              )

              # Simulate some actions
              for i in range(100):
                  result = gov.process_action(
                      agent_id=f"agent_{i % 10}",
                      action=f"test action {i}",
                      cohort="test"
                  )

              end_memory = process.memory_info().rss / 1024 / 1024  # MB
              memory_increase = end_memory - start_memory

              print(f"\nMemory Usage:")
              print(f"  Start: {start_memory:.2f} MB")
              print(f"  End: {end_memory:.2f} MB")
              print(f"  Increase: {memory_increase:.2f} MB")

              return memory_increase

          if __name__ == '__main__':
              memory_used = test_governance_memory()

              # Check for memory leaks (>100 MB for 100 actions is concerning)
              if memory_used > 100:
                  print(f"⚠️  WARNING: High memory usage detected: {memory_used:.2f} MB")
              else:
                  print(f"✅ Memory usage is acceptable: {memory_used:.2f} MB")
          EOF

          python memory_test.py > memory_profile.txt 2>&1 || true

          # Display results
          cat memory_profile.txt

      - name: Upload memory profile
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile
          path: memory_profile.txt
        if: always()

  benchmark-history:
    name: Track Performance History
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Run benchmark
        run: |
          if [ -f "examples/perf/generate_load.py" ]; then
            python examples/perf/generate_load.py \
              --agents 100 \
              --rps 50 \
              --duration 30 \
              --cohort benchmark \
              --output perf_results.csv
          fi

      - name: Store benchmark results
        run: |
          mkdir -p benchmark-history
          timestamp=$(date +%Y%m%d_%H%M%S)
          commit_sha="${{ github.sha }}"

          if [ -f "perf_results.csv" ]; then
            # Add metadata to results (consider using a separate metadata file in future)
            echo "timestamp,commit,branch" > benchmark-history/benchmark_${timestamp}.csv
            echo "$timestamp,$commit_sha,main" >> benchmark-history/benchmark_${timestamp}.csv
            cat perf_results.csv >> benchmark-history/benchmark_${timestamp}.csv
          fi

      - name: Upload to artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-history-${{ github.sha }}
          path: benchmark-history/
        if: always()
